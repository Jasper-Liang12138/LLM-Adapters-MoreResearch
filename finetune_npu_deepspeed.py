import os
import sys

# ### NPU ç¯å¢ƒå˜é‡è®¾ç½®
os.environ["ACL_PRECISION_MODE"] = "must_keep_origin_dtype"
os.environ["LCCL_DETERMINISTIC"] = "1"
os.environ["HCC_DETERMINISTIC"] = "1"
os.environ["PYTHONWARNINGS"] = "ignore"
os.environ["TOKENIZERS_PARALLELISM"] = "false"

from typing import List, Optional, Union
import json
import fire
import torch
import torch_npu
from torch_npu.contrib import transfer_to_npu
import time

# NPU è®¾ç½®
torch_npu.npu.set_compile_mode(jit_compile=False)
torch.npu.set_option({"ACL_PRECISION_MODE": "must_keep_origin_dtype"})

import transformers
from datasets import load_dataset, concatenate_datasets
from tqdm import tqdm

sys.path.append(os.path.join(os.getcwd(), "peft/src/"))
from peft import (
    LoraConfig, get_peft_model
)
from transformers import AutoModelForCausalLM, AutoTokenizer

def safe_list(value, default=None):
    if default is None: default = []
    if value is None: return default
    if isinstance(value, list): return value
    return default

def train(
        base_model: str = "",
        data_path: str = "",
        output_dir: str = "./qwen32b-lora",
        adapter_name: str = "lora",
        batch_size: int = 128,
        micro_batch_size: int = 1,  # ZeRO-3 å»ºè®®ä½¿ç”¨æ›´å°çš„ micro_batch_size
        num_epochs: int = 3,
        learning_rate: float = 2e-5,  # å¤§æ¨¡å‹å»ºè®®ä½¿ç”¨æ›´å°çš„å­¦ä¹ ç‡
        cutoff_len: int = 2048,
        val_set_size: int = 0,
        save_step: int = 500,
        lora_r: int = 64,  # 32B æ¨¡å‹å»ºè®®ä½¿ç”¨æ›´å¤§çš„ rank
        lora_alpha: int = 128,
        lora_dropout: float = 0.05,
        target_modules: List[str] = None,
        train_on_inputs: bool = False,  # é€šå¸¸ä¸è®­ç»ƒ instruction éƒ¨åˆ†
        resume_from_checkpoint: str = None,
        deepspeed_config: str = None,  # DeepSpeed é…ç½®æ–‡ä»¶è·¯å¾„
        # === Curriculum Learning å‚æ•° ===
        use_curriculum: bool = True,  # æ˜¯å¦å¯ç”¨è¯¾ç¨‹å­¦ä¹ 
        curriculum_seed: int = 42,  # æ•°æ®æ··åˆçš„éšæœºç§å­
        local_rank: int = -1,  # DeepSpeed ä¼šè‡ªåŠ¨è®¾ç½®
):
    """
    ä½¿ç”¨ DeepSpeed ZeRO-3 è®­ç»ƒ Qwen-32B
    æ”¯æŒå¤šèŠ‚ç‚¹åˆ†å¸ƒå¼è®­ç»ƒ
    """
    print(f"ğŸš€ Finetuning Qwen-32B on Ascend NPU with DeepSpeed ZeRO-3...")

    # DeepSpeed ä¼šè‡ªåŠ¨è®¾ç½®è¿™äº›ç¯å¢ƒå˜é‡
    local_rank = int(os.environ.get("LOCAL_RANK", "0"))
    world_size = int(os.environ.get("WORLD_SIZE", 1))
    rank = int(os.environ.get("RANK", "0"))

    print(f"ğŸ“Š Distributed Info: Rank {rank}/{world_size}, Local Rank {local_rank}")

    # è®¾ç½®å½“å‰è¿›ç¨‹ä½¿ç”¨çš„ NPU
    torch.npu.set_device(local_rank)

    # æ¸…ç†ç¼“å­˜
    torch.npu.empty_cache()

    # å¤šèŠ‚ç‚¹è®­ç»ƒæ—¶ï¼Œé”™å¼€æ¨¡å‹åŠ è½½æ—¶é—´
    if world_size > 1 and rank > 0:
        time.sleep(rank * 2)

    print(f"ğŸ’¾ Loading model: {base_model}")

    # ä½¿ç”¨ DeepSpeed æ—¶ï¼Œæ¨¡å‹åˆå§‹åŒ–åœ¨ CPU ä¸Šï¼ŒDeepSpeed ä¼šå¤„ç†åˆ†ç‰‡
    model = AutoModelForCausalLM.from_pretrained(
        base_model,
        torch_dtype=torch.bfloat16,
        trust_remote_code=True,
        attn_implementation="eager",  # flash_attention_2 åœ¨ NPU ä¸Šå¯èƒ½ä¸ç¨³å®š
        use_cache=False,  # è®­ç»ƒæ—¶å¿…é¡»ç¦ç”¨
    )

    print(f"âœ… Model loaded on rank {rank}")

    # Tokenizer
    tokenizer = AutoTokenizer.from_pretrained(base_model, trust_remote_code=True)
    tokenizer.pad_token = tokenizer.eos_token
    tokenizer.padding_side = "left"  # å·¦å¡«å……ï¼Œé€‚åˆç”Ÿæˆä»»åŠ¡

    def generate_and_tokenize_prompt(data_point):
        """æ•°æ®é¢„å¤„ç†å‡½æ•°"""
        messages = [
            {"role": "system", "content": "You are a helpful assistant."},
            {"role": "user", "content": data_point["instruction"] + ("\n" + data_point["input"] if data_point.get("input") else "")},
            {"role": "assistant", "content": data_point["output"]}
        ]

        full_tokens = tokenizer.apply_chat_template(
            messages,
            tokenize=True,
            truncation=True,
            max_length=cutoff_len
        )
        labels = list(full_tokens)

        if not train_on_inputs:
            # åªè®¡ç®— assistant éƒ¨åˆ†çš„ loss
            user_tokens = tokenizer.apply_chat_template(
                messages[:-1],
                tokenize=True,
                add_generation_prompt=True
            )
            user_len = len(user_tokens)
            labels = [-100] * user_len + labels[user_len:]
            if len(labels) > len(full_tokens):
                labels = labels[:len(full_tokens)]

        return {
            "input_ids": full_tokens,
            "attention_mask": [1] * len(full_tokens),
            "labels": labels
        }

    # === Curriculum Learning Helper Functions ===
    def get_curriculum_probs(progress):
        """
        æ ¹æ®è®­ç»ƒè¿›åº¦è¿”å›ä¸åŒéš¾åº¦æ•°æ®çš„é‡‡æ ·æ¦‚ç‡
        progress: 0.0 åˆ° 1.0 ä¹‹é—´çš„è®­ç»ƒè¿›åº¦
        è¿”å›: [explain_prob, reasoning_prob, topology_prob]
        """
        if progress < 0.3:
            # æ—©æœŸé˜¶æ®µï¼šä¸»è¦å­¦ä¹ è§£é‡Šæ€§å†…å®¹
            return [0.8, 0.2, 0.0]
        elif progress < 0.7:
            # ä¸­æœŸé˜¶æ®µï¼šå¹³è¡¡è§£é‡Šå’Œæ¨ç†
            return [0.4, 0.4, 0.2]
        else:
            # åæœŸé˜¶æ®µï¼šæ›´å¤šæ¨ç†å’Œæ‹“æ‰‘å†…å®¹
            return [0.25, 0.35, 0.4]

    def build_curriculum_dataset(explain_ds, reasoning_ds, topology_ds, progress):
        """
        æ ¹æ®è®­ç»ƒè¿›åº¦åŠ¨æ€æ„å»ºæ··åˆæ•°æ®é›†
        æ³¨æ„ï¼šè¾“å…¥çš„æ•°æ®é›†åº”è¯¥å·²ç» tokenized
        ä½¿ç”¨ç®€å•çš„é‡‡æ ·ç­–ç•¥ï¼Œé¿å… interleave_datasets çš„å¤æ‚æ€§
        """
        import random

        probs = get_curriculum_probs(progress)
        if rank == 0:  # åªåœ¨ä¸»è¿›ç¨‹æ‰“å°
            print(f"ğŸ“š Curriculum Progress: {progress:.2%} | Sampling Probs: Explain={probs[0]:.2f}, Reasoning={probs[1]:.2f}, Topology={probs[2]:.2f}")

        # ç®€å•ç­–ç•¥ï¼šæŒ‰æ¦‚ç‡è®¡ç®—æ¯ä¸ªæ•°æ®é›†åº”è¯¥å–å¤šå°‘æ ·æœ¬
        total_samples = len(explain_ds) + len(reasoning_ds) + len(topology_ds)

        # è®¡ç®—æ¯ä¸ªæ•°æ®é›†çš„ç›®æ ‡æ ·æœ¬æ•°
        n_explain = int(total_samples * probs[0])
        n_reasoning = int(total_samples * probs[1])
        n_topology = int(total_samples * probs[2])

        # ç¡®ä¿æ€»æ•°æ­£ç¡®ï¼ˆå¤„ç†èˆå…¥è¯¯å·®ï¼‰
        diff = total_samples - (n_explain + n_reasoning + n_topology)
        if diff > 0:
            n_explain += diff

        if rank == 0:
            print(f"ğŸ”„ Building curriculum dataset: {n_explain} explain + {n_reasoning} reasoning + {n_topology} topology = {n_explain + n_reasoning + n_topology} samples")

        # ä»æ¯ä¸ªæ•°æ®é›†ä¸­éšæœºé‡‡æ ·ï¼ˆå…è®¸é‡å¤é‡‡æ ·ä»¥è¾¾åˆ°ç›®æ ‡æ•°é‡ï¼‰
        random.seed(curriculum_seed + int(progress * 1000))  # æ¯ä¸ª epoch ä¸åŒçš„ç§å­

        sampled_datasets = []
        if n_explain > 0:
            # å¦‚æœéœ€è¦çš„æ ·æœ¬æ•°è¶…è¿‡æ•°æ®é›†å¤§å°ï¼Œå…è®¸é‡å¤é‡‡æ ·
            if n_explain <= len(explain_ds):
                indices = random.sample(range(len(explain_ds)), n_explain)
            else:
                # é‡å¤é‡‡æ ·ï¼šå…ˆå…¨éƒ¨å–ï¼Œç„¶åéšæœºè¡¥å……
                indices = list(range(len(explain_ds)))
                indices += random.choices(range(len(explain_ds)), k=n_explain - len(explain_ds))
            sampled_datasets.append(explain_ds.select(indices))
        if n_reasoning > 0:
            if n_reasoning <= len(reasoning_ds):
                indices = random.sample(range(len(reasoning_ds)), n_reasoning)
            else:
                indices = list(range(len(reasoning_ds)))
                indices += random.choices(range(len(reasoning_ds)), k=n_reasoning - len(reasoning_ds))
            sampled_datasets.append(reasoning_ds.select(indices))
        if n_topology > 0:
            if n_topology <= len(topology_ds):
                indices = random.sample(range(len(topology_ds)), n_topology)
            else:
                indices = list(range(len(topology_ds)))
                indices += random.choices(range(len(topology_ds)), k=n_topology - len(topology_ds))
            sampled_datasets.append(topology_ds.select(indices))

        # åˆå¹¶æ‰€æœ‰é‡‡æ ·çš„æ•°æ®é›†
        mixed_ds = concatenate_datasets(sampled_datasets)

        if rank == 0:
            print(f"âœ… Curriculum dataset built: {len(mixed_ds)} samples")

        return mixed_ds

    # LoRA é…ç½®
    target_modules = safe_list(target_modules, [
        "q_proj", "k_proj", "v_proj", "o_proj",
        "gate_proj", "up_proj", "down_proj"  # Qwen çš„ MLP å±‚
    ])

    config = LoraConfig(
        r=lora_r,
        lora_alpha=lora_alpha,
        target_modules=target_modules,
        lora_dropout=lora_dropout,
        bias="none",
        task_type="CAUSAL_LM",
    )

    model = get_peft_model(model, config)
    model.print_trainable_parameters()

    # åŠ è½½æ•°æ®é›†
    print(f"ğŸ“š Loading dataset from {data_path}")
    data = load_dataset("json", data_files=data_path) if data_path.endswith(".json") else load_dataset(data_path)

    # === Curriculum Learning Data Preparation ===
    explain_ds = None
    reasoning_ds = None
    topology_ds = None

    if use_curriculum:
        if rank == 0:
            print("\n" + "="*50)
            print("ğŸ“š Starting Curriculum Learning Setup")
            print("Filtering dataset by difficulty levels...")
            print("="*50 + "\n")

        full_ds = data["train"]

        # æ£€æŸ¥æ•°æ®é›†ä¸­æ˜¯å¦æœ‰ difficulty å­—æ®µ
        if "difficulty" not in full_ds.column_names:
            if rank == 0:
                print("âš ï¸  Warning: 'difficulty' field not found in dataset. Disabling curriculum learning.")
            use_curriculum = False
            train_data = full_ds.shuffle().map(
                generate_and_tokenize_prompt,
                batched=False,
                num_proc=4,
                desc="Tokenizing"
            )
        else:
            # æŒ‰éš¾åº¦è¿‡æ»¤æ•°æ®é›†
            explain_ds = full_ds.filter(lambda x: x.get("difficulty") == "explain")
            reasoning_ds = full_ds.filter(lambda x: x.get("difficulty") == "reasoning")
            topology_ds = full_ds.filter(lambda x: x.get("difficulty") == "topology")

            # å¦‚æœæ²¡æœ‰ topology æ•°æ®ï¼Œä½¿ç”¨ reasoning çš„ä¸€éƒ¨åˆ†
            if len(topology_ds) == 0:
                if rank == 0:
                    print("âš ï¸  No 'topology' difficulty data found. Using reasoning data for topology stage.")
                topology_ds = reasoning_ds

            if rank == 0:
                print(f"âœ… Dataset split by difficulty:")
                print(f"   - Explain: {len(explain_ds)} samples")
                print(f"   - Reasoning: {len(reasoning_ds)} samples")
                print(f"   - Topology: {len(topology_ds)} samples")
                print(f"   - Total: {len(full_ds)} samples\n")

            # å…³é”®ä¼˜åŒ–ï¼šå…ˆ tokenize å„ä¸ªå­æ•°æ®é›†ï¼Œå† interleave
            # è¿™æ ·å¯ä»¥æ˜¾ç¤ºè¿›åº¦æ¡ï¼Œè€Œä¸”åªéœ€è¦ tokenize ä¸€æ¬¡
            if rank == 0:
                print(f"ğŸ”„ Pre-tokenizing datasets (this will be done once)...")
            explain_ds = explain_ds.map(generate_and_tokenize_prompt, batched=False, num_proc=4, desc="Tokenizing Explain")
            reasoning_ds = reasoning_ds.map(generate_and_tokenize_prompt, batched=False, num_proc=4, desc="Tokenizing Reasoning")
            topology_ds = topology_ds.map(generate_and_tokenize_prompt, batched=False, num_proc=4, desc="Tokenizing Topology")
            if rank == 0:
                print(f"âœ… Pre-tokenization complete!\n")
    else:
        # é curriculum æ¨¡å¼ï¼šç›´æ¥å¤„ç†æ•°æ®
        if rank == 0:
            print(f"ğŸ”„ Tokenizing dataset...")
        train_data = data["train"].shuffle().map(
            generate_and_tokenize_prompt,
            batched=False,
            num_proc=4,
            desc="Tokenizing"
        )
        if rank == 0:
            print(f"âœ… Tokenization complete! Total samples: {len(train_data)}")

    # è®¡ç®—æ¢¯åº¦ç´¯ç§¯æ­¥æ•°
    gradient_accumulation_steps = batch_size // (micro_batch_size * world_size)

    if rank == 0:
        print(f"âš™ï¸  Training Configuration:")
        print(f"   - World Size: {world_size}")
        print(f"   - Micro Batch Size: {micro_batch_size}")
        print(f"   - Gradient Accumulation Steps: {gradient_accumulation_steps}")
        print(f"   - Effective Batch Size: {micro_batch_size * gradient_accumulation_steps * world_size}")

    # æ ¹æ®æ˜¯å¦ä½¿ç”¨ curriculum learning å†³å®šåˆå§‹è®­ç»ƒæ•°æ®
    if use_curriculum:
        # Curriculum learning: å…ˆç”¨ç¬¬ä¸€ä¸ª epoch çš„æ•°æ®ï¼ˆprogress=0ï¼‰
        initial_progress = 0.0
        initial_mixed_ds = build_curriculum_dataset(explain_ds, reasoning_ds, topology_ds, initial_progress)
        # æ•°æ®å·²ç» tokenizedï¼Œåªéœ€è¦ shuffle
        initial_train_data = initial_mixed_ds.shuffle()
    else:
        initial_train_data = train_data

    # DeepSpeed é…ç½®
    if deepspeed_config is None:
        # å¦‚æœæ²¡æœ‰æä¾›é…ç½®æ–‡ä»¶ï¼Œä½¿ç”¨é»˜è®¤é…ç½®
        deepspeed_config = {
            "train_batch_size": batch_size,
            "train_micro_batch_size_per_gpu": micro_batch_size,
            "gradient_accumulation_steps": gradient_accumulation_steps,
            "gradient_clipping": 1.0,
            "zero_optimization": {
                "stage": 3,
                "offload_optimizer": {
                    "device": "cpu",
                    "pin_memory": True
                },
                "offload_param": {
                    "device": "cpu",
                    "pin_memory": True
                },
                "overlap_comm": True,
                "contiguous_gradients": True,
                "sub_group_size": 1e9,
                "reduce_bucket_size": 5e8,
                "stage3_prefetch_bucket_size": 5e8,
                "stage3_param_persistence_threshold": 1e6,
                "stage3_max_live_parameters": 1e9,
                "stage3_max_reuse_distance": 1e9,
                "stage3_gather_16bit_weights_on_model_save": True
            },
            "bf16": {
                "enabled": True
            },
            "steps_per_print": 10,
            "wall_clock_breakdown": False
        }

        # ä¿å­˜é…ç½®åˆ°æ–‡ä»¶
        ds_config_path = os.path.join(output_dir, "ds_config.json")
        os.makedirs(output_dir, exist_ok=True)
        with open(ds_config_path, "w") as f:
            json.dump(deepspeed_config, f, indent=2)
        print(f"ğŸ’¾ DeepSpeed config saved to {ds_config_path}")
        deepspeed_config = ds_config_path

    # è®­ç»ƒå‚æ•°
    training_args = transformers.TrainingArguments(
        output_dir=output_dir,
        per_device_train_batch_size=micro_batch_size,
        gradient_accumulation_steps=gradient_accumulation_steps,
        num_train_epochs=1 if use_curriculum else num_epochs,  # Curriculum: æ¯æ¬¡è®­ç»ƒ1ä¸ªepoch
        learning_rate=learning_rate,
        bf16=True,
        fp16=False,
        logging_steps=10,
        logging_first_step=True,
        save_strategy="steps",
        save_steps=save_step,
        save_total_limit=3,
        dataloader_pin_memory=False,
        dataloader_num_workers=4,
        remove_unused_columns=False,
        ddp_find_unused_parameters=False,
        deepspeed=deepspeed_config,  # å¯ç”¨ DeepSpeed
        report_to="none",
        warmup_steps=100,
        lr_scheduler_type="cosine",
        weight_decay=0.01,
        max_grad_norm=1.0,
    )

    # Trainer
    trainer = transformers.Trainer(
        model=model,
        args=training_args,
        train_dataset=initial_train_data,
        data_collator=transformers.DataCollatorForSeq2Seq(
            tokenizer,
            pad_to_multiple_of=8,
            return_tensors="pt",
            padding=True
        ),
    )

    # å¼€å§‹è®­ç»ƒ
    if rank == 0:
        print(f"\n{'='*60}")
        print(f"ğŸ¯ Starting Training...")
        print(f"{'='*60}\n")

    # =================================================================
    # Training Loop with Curriculum Learning Support
    # =================================================================
    if use_curriculum:
        if rank == 0:
            print("\n" + "="*50)
            print("ğŸ“ Starting Curriculum Learning Training")
            print(f"Total Epochs: {num_epochs}")
            print("="*50 + "\n")

        for epoch in range(num_epochs):
            # è®¡ç®—å½“å‰è®­ç»ƒè¿›åº¦
            progress = epoch / num_epochs

            if rank == 0:
                print(f"\n{'='*50}")
                print(f"ğŸ“– Epoch {epoch + 1}/{num_epochs} (Progress: {progress:.2%})")
                print(f"{'='*50}\n")

            # åŠ¨æ€æ„å»ºå½“å‰ epoch çš„æ•°æ®é›†
            mixed_ds = build_curriculum_dataset(explain_ds, reasoning_ds, topology_ds, progress)
            # æ•°æ®å·²ç» tokenizedï¼Œåªéœ€è¦ shuffle
            current_train_data = mixed_ds.shuffle()

            # æ›´æ–° trainer çš„è®­ç»ƒæ•°æ®é›†
            trainer.train_dataset = current_train_data

            # è®­ç»ƒå½“å‰ epoch
            if epoch == 0:
                # ç¬¬ä¸€ä¸ª epochï¼Œå¯èƒ½éœ€è¦ä» checkpoint æ¢å¤
                trainer.train(resume_from_checkpoint=resume_from_checkpoint)
            else:
                # åç»­ epochï¼šä¸ä» checkpoint æ¢å¤ï¼Œç›´æ¥ç»§ç»­è®­ç»ƒ
                # æ¨¡å‹å·²ç»åœ¨å†…å­˜ä¸­å¹¶ä¸”å·²è®­ç»ƒï¼Œä¸éœ€è¦é‡æ–°åŠ è½½
                trainer.train(resume_from_checkpoint=None)

            if rank == 0:
                print(f"\nâœ… Epoch {epoch + 1}/{num_epochs} completed!\n")

        if rank == 0:
            print("\n" + "="*50)
            print("ğŸ‰ Curriculum Learning Training Completed!")
            print("="*50 + "\n")
    else:
        # æ ‡å‡†è®­ç»ƒï¼ˆä¸ä½¿ç”¨ curriculum learningï¼‰
        trainer.train(resume_from_checkpoint=resume_from_checkpoint)

    # ä¿å­˜æ¨¡å‹ï¼ˆåªåœ¨ rank 0 ä¿å­˜ï¼‰
    if rank == 0:
        print(f"\nğŸ’¾ Saving model to {output_dir}")
        model.save_pretrained(output_dir)
        tokenizer.save_pretrained(output_dir)
        print(f"âœ… Training completed!")

if __name__ == "__main__":
    fire.Fire(train)
