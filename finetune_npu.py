import os
import sys

# ### ä¿®æ”¹ç‚¹ 1: åœ¨å¯¼å…¥ torch ä¹‹å‰è®¾ç½®ç¯å¢ƒå˜é‡
# å¼ºåˆ¶ NPU ä¿æŒåŸå›¾æ•°æ®ç±»å‹ï¼Œç¦æ­¢ç§è‡ªè½¬ FP16ï¼Œè¿™æ˜¯è§£å†³æº¢å‡ºçš„å…³é”®
os.environ["ACL_PRECISION_MODE"] = "must_keep_origin_dtype"
# ä¼˜åŒ–æ˜¾å­˜åˆ†é…ï¼Œé˜²æ­¢ç¢ç‰‡åŒ–
os.environ["PYTORCH_NPU_ALLOC_CONF"] = "max_split_size_mb:128"
# ç¦æ­¢æŸäº›å¯èƒ½å¯¼è‡´ Inner Error çš„èåˆç®—å­
os.environ["LCCL_DETERMINISTIC"] = "1"
os.environ["HCC_DETERMINISTIC"] = "1"
os.environ["PYTHONWARNINGS"] = "ignore"
# å…³é”®ï¼šæŸäº›ç¯å¢ƒä¸‹ç¦ç”¨è¿™ä¸ªå¯ä»¥è§£å†³ RMSNorm å¯¼è‡´çš„ Inner Error
os.environ["ACL_PRECISION_MODE"] = "must_keep_origin_dtype" 
os.environ["TOKENIZERS_PARALLELISM"] = "false"

from typing import List, Optional, Union
import json
import fire
import torch
import torch_npu  # æ ¸å¿ƒï¼šæ˜‡è…¾å¿…éœ€
from torch_npu.contrib import transfer_to_npu

# ### ä¿®æ”¹ç‚¹ 2: NPU è®¾ç½®
# å…³é—­ JIT ç¼–è¯‘ï¼ˆQwen2.5 åœ¨ NPU ä¸Š JIT æœ‰æ—¶ä¼šä¸ç¨³å®šï¼‰
torch_npu.npu.set_compile_mode(jit_compile=False)
# å†æ¬¡é€šè¿‡ API ç¡®ä¿ç²¾åº¦æ¨¡å¼ï¼ˆåŒé‡ä¿é™©ï¼‰
torch.npu.set_option({"ACL_PRECISION_MODE": "must_keep_origin_dtype"})

import transformers
from datasets import load_dataset, Dataset, interleave_datasets
from tqdm import tqdm

sys.path.append(os.path.join(os.getcwd(), "peft/src/"))
from peft import (
    LoraConfig, AdaLoraConfig, QLoRAConfig, BottleneckConfig, PrefixTuningConfig,
    get_peft_model, get_peft_model_state_dict, set_peft_model_state_dict,
    prepare_model_for_int8_training, prepare_model_for_kbit_training
)
from transformers import AutoModelForCausalLM, AutoTokenizer

# --- å·¥å…·å‡½æ•°ä¿æŒä¸å˜ ---
def safe_int(value, default=0):
    try: return int(value)
    except: return default
def safe_float(value, default=0.0):
    try: return float(value)
    except: return default
def safe_bool(value, default=False):
    if isinstance(value, bool): return value
    if isinstance(value, str): return value.lower() in ('true', '1', 'yes', 'y', 't')
    return bool(value)
def safe_list(value, default=None):
    if default is None: default = []
    if value is None: return default
    if isinstance(value, list): return value
    return default
def safe_str(value, default=""):
    if value is None: return default
    return str(value)

def train(
        base_model: str = "", 
        data_path: str = "yahma/alpaca-cleaned",
        output_dir: str = "./lora-qwen", 
        adapter_name: str = "lora",
        load_8bit: bool = False,
        batch_size: int = 128,
        micro_batch_size: int = 4,
        num_epochs: int = 3,
        learning_rate: float = 3e-4,
        cutoff_len: int = 1024,
        val_set_size: int = 0,
        use_gradient_checkpointing: bool = False,
        eval_step: int = 50,
        save_step: int = 200,
        lora_r: int = 8,
        lora_alpha: int = 16,
        lora_dropout: float = 0.05,
        target_modules: List[str] = None,
        train_on_inputs: bool = True,
        group_by_length: bool = False,
        wandb_project: str = "",
        wandb_run_name: str = "",
        wandb_watch: str = "",
        wandb_log_model: str = "",
        resume_from_checkpoint: str = None,
        use_tf_grpo: bool = False,
        grpo_group_size: int = 4,
        grpo_max_experiences: int = 50,
        grpo_data_limit: int = -1,
        # === Curriculum Learning å‚æ•° ===
        use_curriculum: bool = False,  # æ˜¯å¦å¯ç”¨è¯¾ç¨‹å­¦ä¹ 
        curriculum_seed: int = 42,  # æ•°æ®æ··åˆçš„éšæœºç§å­
):
    print(f"Finetuning Qwen2.5 on Ascend NPU with Full BF16 Precision...")
    
    world_size = int(os.environ.get("WORLD_SIZE", 1))
    local_rank = int(os.environ.get("LOCAL_RANK") or 0)
    device_map = {"": local_rank}

    # ### ä¿®æ”¹ç‚¹ 3: æ˜¾å¼æŒ‡å®š torch_dtype ä¸º bfloat16
    model = AutoModelForCausalLM.from_pretrained(
        base_model,
        load_in_8bit=load_8bit,
        torch_dtype=torch.bfloat16, 
        device_map=device_map,
        trust_remote_code=True,
        attn_implementation="eager" 
    )

    tokenizer = AutoTokenizer.from_pretrained(base_model, trust_remote_code=True)
    tokenizer.pad_token = tokenizer.eos_token
    tokenizer.padding_side = "left"

    def generate_and_tokenize_prompt(data_point):
        messages = [
            {"role": "system", "content": "You are a helpful assistant."},
            {"role": "user", "content": data_point["instruction"] + ("\n" + data_point["input"] if data_point.get("input") else "")},
            {"role": "assistant", "content": data_point["output"]}
        ]
        full_tokens = tokenizer.apply_chat_template(messages, tokenize=True, truncation=True, max_length=cutoff_len)
        labels = list(full_tokens)
        if not train_on_inputs:
            user_tokens = tokenizer.apply_chat_template(messages[:-1], tokenize=True, add_generation_prompt=True)
            user_len = len(user_tokens)
            labels = [-100] * user_len + labels[user_len:]
            if len(labels) > len(full_tokens): labels = labels[:len(full_tokens)]
        return {"input_ids": full_tokens, "attention_mask": [1] * len(full_tokens), "labels": labels}

    # === Curriculum Learning Helper Functions ===
    def get_curriculum_probs(progress):
        """
        æ ¹æ®è®­ç»ƒè¿›åº¦è¿”å›ä¸åŒéš¾åº¦æ•°æ®çš„é‡‡æ ·æ¦‚ç‡
        progress: 0.0 åˆ° 1.0 ä¹‹é—´çš„è®­ç»ƒè¿›åº¦
        è¿”å›: [explain_prob, reasoning_prob, topology_prob]
        """
        if progress < 0.3:
            # æ—©æœŸé˜¶æ®µï¼šä¸»è¦å­¦ä¹ è§£é‡Šæ€§å†…å®¹
            return [0.8, 0.2, 0.0]
        elif progress < 0.7:
            # ä¸­æœŸé˜¶æ®µï¼šå¹³è¡¡è§£é‡Šå’Œæ¨ç†
            return [0.4, 0.4, 0.2]
        else:
            # åæœŸé˜¶æ®µï¼šæ›´å¤šæ¨ç†å’Œæ‹“æ‰‘å†…å®¹
            return [0.25, 0.35, 0.4]

    def build_curriculum_dataset(explain_ds, reasoning_ds, topology_ds, progress):
        """
        æ ¹æ®è®­ç»ƒè¿›åº¦åŠ¨æ€æ„å»ºæ··åˆæ•°æ®é›†
        """
        probs = get_curriculum_probs(progress)
        print(f"ğŸ“š Curriculum Progress: {progress:.2%} | Sampling Probs: Explain={probs[0]:.2f}, Reasoning={probs[1]:.2f}, Topology={probs[2]:.2f}")

        # ä½¿ç”¨ interleave_datasets æŒ‰æ¦‚ç‡æ··åˆæ•°æ®
        mixed_ds = interleave_datasets(
            [explain_ds, reasoning_ds, topology_ds],
            probabilities=probs,
            seed=curriculum_seed,
            stopping_strategy="all_exhausted"
        )

        return mixed_ds

    if load_8bit:
        model = prepare_model_for_int8_training(model, use_gradient_checkpointing=use_gradient_checkpointing)
    elif adapter_name == "qlora":
        model = prepare_model_for_kbit_training(model, use_gradient_checkpointing=use_gradient_checkpointing)
    
    target_modules = safe_list(target_modules, ["q_proj", "k_proj", "v_proj", "o_proj"])
    
    if adapter_name == "lora":
        config = LoraConfig(
            r=lora_r,
            lora_alpha=lora_alpha,
            target_modules=target_modules, 
            lora_dropout=lora_dropout,
            bias="none",
            task_type="CAUSAL_LM",
        )
    elif adapter_name == "adalora":
        config = AdaLoraConfig(
            r=lora_r,
            lora_alpha=lora_alpha,
            target_modules=target_modules,
            lora_dropout=lora_dropout,
            task_type="CAUSAL_LM",
            # ===== AdaLoRA å…³é”®å‚æ•° =====
            init_r=lora_r,
            target_r=lora_r,
            beta1=0.85,
            beta2=0.85,
            tinit=200,
            tfinal=1000,
            deltaT=10,
        )
    elif adapter_name == "qlora":
        config = QLoRAConfig(
            r=lora_r,
            lora_alpha=lora_alpha,
            target_modules=target_modules,
            lora_dropout=lora_dropout,
            task_type="CAUSAL_LM",
        )
    elif adapter_name == "bottleneck":
        config = BottleneckConfig(
            bottleneck_size=bottleneck_size,
            non_linearity=non_linearity,
            adapter_dropout=adapter_dropout,
            use_parallel_adapter=use_parallel_adapter,
            use_adapterp=use_adapterp,
            target_modules=target_modules,
            scaling=scaling,
            bias="none",
            task_type="CAUSAL_LM",
        )
    elif adapter_name == "prefix-tuning":
        config = PrefixTuningConfig(
            num_virtual_tokens=num_virtual_tokens,
            task_type="CAUSAL_LM",
        )
        
    model = get_peft_model(model, config)
    if adapter_name == "prefix-tuning":
        model.to(f'npu:{local_rank}')

    # --- NPU ç¨³å®šæ€§è¡¥ä¸ï¼šä¿®å¤ RMSNorm å¯¼è‡´çš„ Inner Error ---
    from transformers.models.qwen2.modeling_qwen2 import Qwen2RMSNorm

    for name, module in model.named_modules():
        if isinstance(module, Qwen2RMSNorm):
            # å¼ºåˆ¶è¿™äº›å±‚åœ¨æ­£å‘ä¼ æ’­æ—¶è½¬æ¢ç±»å‹ï¼Œè§„é¿ NPU ç®—å­ bug
            module.float() 
    # -----------------------------------------------------

    # æ‰“å°å¯è®­ç»ƒå‚æ•°ï¼Œç¡®è®¤ LoRA æŒ‚è½½æ­£ç¡®
    model.print_trainable_parameters()

    # ### ä¿®æ”¹ç‚¹ 4: ç§»é™¤æ‰‹åŠ¨çš„ model.bfloat16()ï¼Œä¾èµ– Trainer çš„å‚æ•°æ§åˆ¶
    # å› ä¸º Trainer ä¼šæ ¹æ® args.bf16 è‡ªåŠ¨å¤„ç†ï¼Œæ‰‹åŠ¨è½¬æ¢æœ‰æ—¶ä¼šæ‰°ä¹± Trainer çš„çŠ¶æ€
    
    data = load_dataset("json", data_files=data_path) if data_path.endswith(".json") else load_dataset(data_path)

    # === Curriculum Learning Data Preparation ===
    explain_ds = None
    reasoning_ds = None
    topology_ds = None

    if use_curriculum:
        print("\n" + "="*50)
        print("ğŸ“š Starting Curriculum Learning Setup")
        print("Filtering dataset by difficulty levels...")
        print("="*50 + "\n")

        full_ds = data["train"]

        # æ£€æŸ¥æ•°æ®é›†ä¸­æ˜¯å¦æœ‰ difficulty å­—æ®µ
        if "difficulty" not in full_ds.column_names:
            print("âš ï¸  Warning: 'difficulty' field not found in dataset. Disabling curriculum learning.")
            use_curriculum = False
            train_data = full_ds.shuffle().map(generate_and_tokenize_prompt)
        else:
            # æŒ‰éš¾åº¦è¿‡æ»¤æ•°æ®é›†
            explain_ds = full_ds.filter(lambda x: x.get("difficulty") == "explain")
            reasoning_ds = full_ds.filter(lambda x: x.get("difficulty") == "reasoning")
            topology_ds = full_ds.filter(lambda x: x.get("difficulty") == "topology")

            # å¦‚æœæ²¡æœ‰ topology æ•°æ®ï¼Œä½¿ç”¨ reasoning çš„ä¸€éƒ¨åˆ†
            if len(topology_ds) == 0:
                print("âš ï¸  No 'topology' difficulty data found. Using reasoning data for topology stage.")
                topology_ds = reasoning_ds

            print(f"âœ… Dataset split by difficulty:")
            print(f"   - Explain: {len(explain_ds)} samples")
            print(f"   - Reasoning: {len(reasoning_ds)} samples")
            print(f"   - Topology: {len(topology_ds)} samples")
            print(f"   - Total: {len(full_ds)} samples\n")
    else:
        train_data = data["train"].shuffle().map(generate_and_tokenize_prompt)

    gradient_accumulation_steps = (batch_size // micro_batch_size) // world_size

    # æ ¹æ®æ˜¯å¦ä½¿ç”¨ curriculum learning å†³å®šåˆå§‹è®­ç»ƒæ•°æ®
    if use_curriculum:
        # Curriculum learning: å…ˆç”¨ç¬¬ä¸€ä¸ª epoch çš„æ•°æ®ï¼ˆprogress=0ï¼‰
        initial_progress = 0.0
        initial_mixed_ds = build_curriculum_dataset(explain_ds, reasoning_ds, topology_ds, initial_progress)
        initial_train_data = initial_mixed_ds.shuffle().map(generate_and_tokenize_prompt)
    else:
        initial_train_data = train_data

    # ### ä¿®æ”¹ç‚¹ 5: æ ¸å¿ƒä¿®å¤ - å¼€å¯ bf16=True
    # è¿™ä¼šå‘Šè¯‰ Trainer ä¸è¦ä½¿ç”¨ GradScalerï¼Œå› ä¸º BF16 ä¸éœ€è¦ç¼©æ”¾ã€‚
    # å½»åº•è§£å†³ "Loss scaler reducing loss scale to 0.0" é—®é¢˜

    trainer = transformers.Trainer(
        model=model,
        train_dataset=initial_train_data,
        args=transformers.TrainingArguments(
            per_device_train_batch_size=micro_batch_size,
            gradient_accumulation_steps=gradient_accumulation_steps,
            warmup_steps=100,
            num_train_epochs=1 if use_curriculum else num_epochs,  # Curriculum: æ¯æ¬¡è®­ç»ƒ1ä¸ªepoch
            learning_rate=learning_rate,

            # --- å…³é”®ä¿®æ”¹ ---
            bf16=True,      # å¿…é¡»å¼€å¯ï¼è¿™ä¼šç¦ç”¨ FP16 GradScaler
            fp16=False,     # ç¡®ä¿å…³é—­ FP16
            optim="adamw_torch",
            # ----------------

            dataloader_pin_memory=False,
            logging_steps=10,
            save_strategy="steps",
            save_steps=save_step,
            output_dir=output_dir,
            ddp_find_unused_parameters=False,
            report_to="none",

        ),
        data_collator=transformers.DataCollatorForSeq2Seq(tokenizer, pad_to_multiple_of=8, padding=True),
    )
    # é˜²æ­¢ Qwen çš„ token_type_ids å¯¼è‡´ NaN (Qwen ä¸éœ€è¦è¿™ä¸ªï¼Œä½†æœ‰æ—¶ä¼šè¢«è‡ªåŠ¨åŠ ä¸Š)
    if hasattr(model, "config"):
        model.config.use_cache = False

    # å¼ºåˆ¶æ¸…ç†ä¸€ä¸‹å†…å­˜
    torch.npu.empty_cache()

    model.config.use_cache = False

    # =================================================================
    # Training Loop with Curriculum Learning Support
    # =================================================================
    if use_curriculum:
        print("\n" + "="*50)
        print("ğŸ“ Starting Curriculum Learning Training")
        print(f"Total Epochs: {num_epochs}")
        print("="*50 + "\n")

        for epoch in range(num_epochs):
            # è®¡ç®—å½“å‰è®­ç»ƒè¿›åº¦
            progress = epoch / num_epochs

            print(f"\n{'='*50}")
            print(f"ğŸ“– Epoch {epoch + 1}/{num_epochs} (Progress: {progress:.2%})")
            print(f"{'='*50}\n")

            # åŠ¨æ€æ„å»ºå½“å‰ epoch çš„æ•°æ®é›†
            mixed_ds = build_curriculum_dataset(explain_ds, reasoning_ds, topology_ds, progress)
            current_train_data = mixed_ds.shuffle().map(generate_and_tokenize_prompt)

            # æ›´æ–° trainer çš„è®­ç»ƒæ•°æ®é›†
            trainer.train_dataset = current_train_data

            # è®­ç»ƒå½“å‰ epoch
            if epoch == 0:
                # ç¬¬ä¸€ä¸ª epochï¼Œå¯èƒ½éœ€è¦ä» checkpoint æ¢å¤
                trainer.train(resume_from_checkpoint=resume_from_checkpoint)
            else:
                # åç»­ epochï¼Œä»ä¸Šä¸€ä¸ª epoch çš„ç»“æœç»§ç»­
                trainer.train(resume_from_checkpoint=True)

            print(f"\nâœ… Epoch {epoch + 1}/{num_epochs} completed!\n")

        print("\n" + "="*50)
        print("ğŸ‰ Curriculum Learning Training Completed!")
        print("="*50 + "\n")
    else:
        # æ ‡å‡†è®­ç»ƒï¼ˆä¸ä½¿ç”¨ curriculum learningï¼‰
        trainer.train(resume_from_checkpoint=resume_from_checkpoint)

    model.save_pretrained(output_dir)

if __name__ == "__main__":
    fire.Fire(train)