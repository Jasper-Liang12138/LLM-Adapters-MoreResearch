import os
import sys

# ### ä¿®æ”¹ç‚¹ 1: åœ¨å¯¼å…¥ torch ä¹‹å‰è®¾ç½®ç¯å¢ƒå˜é‡
# å¼ºåˆ¶ NPU ä¿æŒåŸå›¾æ•°æ®ç±»å‹ï¼Œç¦æ­¢ç§è‡ªè½¬ FP16ï¼Œè¿™æ˜¯è§£å†³æº¢å‡ºçš„å…³é”®
os.environ["ACL_PRECISION_MODE"] = "must_keep_origin_dtype"
# ç¦æ­¢æŸäº›å¯èƒ½å¯¼è‡´ Inner Error çš„èåˆç®—å­
os.environ["LCCL_DETERMINISTIC"] = "1"
os.environ["HCC_DETERMINISTIC"] = "1"
os.environ["PYTHONWARNINGS"] = "ignore"
# å…³é”®ï¼šæŸäº›ç¯å¢ƒä¸‹ç¦ç”¨è¿™ä¸ªå¯ä»¥è§£å†³ RMSNorm å¯¼è‡´çš„ Inner Error
os.environ["ACL_PRECISION_MODE"] = "must_keep_origin_dtype" 
os.environ["TOKENIZERS_PARALLELISM"] = "false"

from typing import List, Optional, Union
import json
import fire
import torch
import torch_npu  # æ ¸å¿ƒï¼šæ˜‡è…¾å¿…éœ€
from torch_npu.contrib import transfer_to_npu
import time  # ç”¨äºè¿›ç¨‹é—´å»¶è¿ŸåŠ è½½

# ### ä¿®æ”¹ç‚¹ 2: NPU è®¾ç½®
# å…³é—­ JIT ç¼–è¯‘ï¼ˆQwen2.5 åœ¨ NPU ä¸Š JIT æœ‰æ—¶ä¼šä¸ç¨³å®šï¼‰
torch_npu.npu.set_compile_mode(jit_compile=False)
# å†æ¬¡é€šè¿‡ API ç¡®ä¿ç²¾åº¦æ¨¡å¼ï¼ˆåŒé‡ä¿é™©ï¼‰
torch.npu.set_option({"ACL_PRECISION_MODE": "must_keep_origin_dtype"})

import transformers
from datasets import load_dataset, concatenate_datasets
from tqdm import tqdm

sys.path.append(os.path.join(os.getcwd(), "peft/src/"))
from peft import (
    LoraConfig, AdaLoraConfig, BottleneckConfig, PrefixTuningConfig,
    get_peft_model, get_peft_model_state_dict, set_peft_model_state_dict
)
from transformers import AutoModelForCausalLM, AutoTokenizer

# --- å·¥å…·å‡½æ•°ä¿æŒä¸å˜ ---
def safe_int(value, default=0):
    try: return int(value)
    except: return default
def safe_float(value, default=0.0):
    try: return float(value)
    except: return default
def safe_bool(value, default=False):
    if isinstance(value, bool): return value
    if isinstance(value, str): return value.lower() in ('true', '1', 'yes', 'y', 't')
    return bool(value)
def safe_list(value, default=None):
    if default is None: default = []
    if value is None: return default
    if isinstance(value, list): return value
    return default
def safe_str(value, default=""):
    if value is None: return default
    return str(value)

def train(
        base_model: str = "",
        data_path: str = "yahma/alpaca-cleaned",
        output_dir: str = "./lora-qwen",
        adapter_name: str = "lora",
        batch_size: int = 128,
        micro_batch_size: int = 4,
        num_epochs: int = 3,
        learning_rate: float = 3e-4,
        cutoff_len: int = 1024,
        val_set_size: int = 0,
        eval_step: int = 50,
        save_step: int = 200,
        lora_r: int = 8,
        lora_alpha: int = 16,
        lora_dropout: float = 0.05,
        target_modules: List[str] = None,
        train_on_inputs: bool = True,
        group_by_length: bool = False,
        wandb_project: str = "",
        wandb_run_name: str = "",
        wandb_watch: str = "",
        wandb_log_model: str = "",
        resume_from_checkpoint: str = None,
        use_tf_grpo: bool = False,
        grpo_group_size: int = 4,
        grpo_max_experiences: int = 50,
        grpo_data_limit: int = -1,
        # === Curriculum Learning å‚æ•° ===
        use_curriculum: bool = True,  # æ˜¯å¦å¯ç”¨è¯¾ç¨‹å­¦ä¹ 
        curriculum_seed: int = 42,  # æ•°æ®æ··åˆçš„éšæœºç§å­
        # === å†…å­˜ä¼˜åŒ–å‚æ•° ===
        use_gradient_checkpointing: bool = True,  # æ¢¯åº¦æ£€æŸ¥ç‚¹ï¼ŒèŠ‚çœæ˜¾å­˜ï¼ˆæ¨èå¼€å¯ï¼‰
):
    print(f"Finetuning Qwen2.5 on Ascend NPU with Full BF16 Precision...")

    world_size = int(os.environ.get("WORLD_SIZE", 1))
    local_rank = int(os.environ.get("LOCAL_RANK", "0"))

    # ç›´æ¥è®¾ç½®å½“å‰è¿›ç¨‹ä½¿ç”¨çš„è®¾å¤‡
    torch.npu.set_device(local_rank)
    device = f'npu:{local_rank}'

    # æ¸…ç†å½“å‰ NPU çš„ç¼“å­˜
    torch.npu.empty_cache()

    # è¿›ç¨‹é—´é”™å¼€åŠ è½½ï¼Œé¿å…åŒæ—¶è¯»å–æ¨¡å‹æ–‡ä»¶å¯¼è‡´ I/O ç“¶é¢ˆ
    if world_size > 1 and local_rank > 0:
        time.sleep(local_rank * 3)

    print(f"Process rank {local_rank}/{world_size}: Loading model to {device}...")

    # ç®€åŒ–åŠ è½½ï¼šç›´æ¥åŠ è½½åˆ°æŒ‡å®šè®¾å¤‡ï¼Œä¸ä½¿ç”¨ device_map å’Œ max_memory
    # è¿™äº›å‚æ•°åœ¨åˆ†å¸ƒå¼è®­ç»ƒæ—¶ä¼šå¯¼è‡´è®¾å¤‡åˆ†é…æ··ä¹±
    model = AutoModelForCausalLM.from_pretrained(
        base_model,
        torch_dtype=torch.bfloat16,
        low_cpu_mem_usage=True,
        trust_remote_code=True,
        attn_implementation="eager"
    )

    print(f"Process rank {local_rank}: Model loaded successfully on CPU, moving to {device}...")

    tokenizer = AutoTokenizer.from_pretrained(base_model, trust_remote_code=True)
    tokenizer.pad_token = tokenizer.eos_token
    tokenizer.padding_side = "left"

    def generate_and_tokenize_prompt(data_point):
        messages = [
            {"role": "system", "content": "You are a helpful assistant."},
            {"role": "user", "content": data_point["instruction"] + ("\n" + data_point["input"] if data_point.get("input") else "")},
            {"role": "assistant", "content": data_point["output"]}
        ]
        full_tokens = tokenizer.apply_chat_template(messages, tokenize=True, truncation=True, max_length=cutoff_len)
        labels = list(full_tokens)
        if not train_on_inputs:
            user_tokens = tokenizer.apply_chat_template(messages[:-1], tokenize=True, add_generation_prompt=True)
            user_len = len(user_tokens)
            labels = [-100] * user_len + labels[user_len:]
            if len(labels) > len(full_tokens): labels = labels[:len(full_tokens)]
        return {"input_ids": full_tokens, "attention_mask": [1] * len(full_tokens), "labels": labels}

    # === Curriculum Learning Helper Functions ===
    def get_curriculum_probs(progress):
        """
        æ ¹æ®è®­ç»ƒè¿›åº¦è¿”å›ä¸åŒéš¾åº¦æ•°æ®çš„é‡‡æ ·æ¦‚ç‡
        progress: 0.0 åˆ° 1.0 ä¹‹é—´çš„è®­ç»ƒè¿›åº¦
        è¿”å›: [explain_prob, reasoning_prob, topology_prob]
        """
        if progress < 0.3:
            # æ—©æœŸé˜¶æ®µï¼šä¸»è¦å­¦ä¹ è§£é‡Šæ€§å†…å®¹
            return [0.8, 0.2, 0.0]
        elif progress < 0.7:
            # ä¸­æœŸé˜¶æ®µï¼šå¹³è¡¡è§£é‡Šå’Œæ¨ç†
            return [0.4, 0.4, 0.2]
        else:
            # åæœŸé˜¶æ®µï¼šæ›´å¤šæ¨ç†å’Œæ‹“æ‰‘å†…å®¹
            return [0.25, 0.35, 0.4]

    def build_curriculum_dataset(explain_ds, reasoning_ds, topology_ds, progress):
        """
        æ ¹æ®è®­ç»ƒè¿›åº¦åŠ¨æ€æ„å»ºæ··åˆæ•°æ®é›†
        æ³¨æ„ï¼šè¾“å…¥çš„æ•°æ®é›†åº”è¯¥å·²ç» tokenized
        ä½¿ç”¨ç®€å•çš„é‡‡æ ·ç­–ç•¥ï¼Œé¿å… interleave_datasets çš„å¤æ‚æ€§
        """
        import random

        probs = get_curriculum_probs(progress)
        print(f"ğŸ“š Curriculum Progress: {progress:.2%} | Sampling Probs: Explain={probs[0]:.2f}, Reasoning={probs[1]:.2f}, Topology={probs[2]:.2f}")

        # ç®€å•ç­–ç•¥ï¼šæŒ‰æ¦‚ç‡è®¡ç®—æ¯ä¸ªæ•°æ®é›†åº”è¯¥å–å¤šå°‘æ ·æœ¬
        total_samples = len(explain_ds) + len(reasoning_ds) + len(topology_ds)

        # è®¡ç®—æ¯ä¸ªæ•°æ®é›†çš„ç›®æ ‡æ ·æœ¬æ•°
        n_explain = int(total_samples * probs[0])
        n_reasoning = int(total_samples * probs[1])
        n_topology = int(total_samples * probs[2])

        # ç¡®ä¿æ€»æ•°æ­£ç¡®ï¼ˆå¤„ç†èˆå…¥è¯¯å·®ï¼‰
        diff = total_samples - (n_explain + n_reasoning + n_topology)
        if diff > 0:
            n_explain += diff

        print(f"ğŸ”„ Building curriculum dataset: {n_explain} explain + {n_reasoning} reasoning + {n_topology} topology = {n_explain + n_reasoning + n_topology} samples")

        # ä»æ¯ä¸ªæ•°æ®é›†ä¸­éšæœºé‡‡æ ·ï¼ˆå…è®¸é‡å¤é‡‡æ ·ä»¥è¾¾åˆ°ç›®æ ‡æ•°é‡ï¼‰
        random.seed(curriculum_seed + int(progress * 1000))  # æ¯ä¸ª epoch ä¸åŒçš„ç§å­

        sampled_datasets = []
        if n_explain > 0:
            # å¦‚æœéœ€è¦çš„æ ·æœ¬æ•°è¶…è¿‡æ•°æ®é›†å¤§å°ï¼Œå…è®¸é‡å¤é‡‡æ ·
            if n_explain <= len(explain_ds):
                indices = random.sample(range(len(explain_ds)), n_explain)
            else:
                # é‡å¤é‡‡æ ·ï¼šå…ˆå…¨éƒ¨å–ï¼Œç„¶åéšæœºè¡¥å……
                indices = list(range(len(explain_ds)))
                indices += random.choices(range(len(explain_ds)), k=n_explain - len(explain_ds))
            sampled_datasets.append(explain_ds.select(indices))
        if n_reasoning > 0:
            if n_reasoning <= len(reasoning_ds):
                indices = random.sample(range(len(reasoning_ds)), n_reasoning)
            else:
                indices = list(range(len(reasoning_ds)))
                indices += random.choices(range(len(reasoning_ds)), k=n_reasoning - len(reasoning_ds))
            sampled_datasets.append(reasoning_ds.select(indices))
        if n_topology > 0:
            if n_topology <= len(topology_ds):
                indices = random.sample(range(len(topology_ds)), n_topology)
            else:
                indices = list(range(len(topology_ds)))
                indices += random.choices(range(len(topology_ds)), k=n_topology - len(topology_ds))
            sampled_datasets.append(topology_ds.select(indices))

        # åˆå¹¶æ‰€æœ‰é‡‡æ ·çš„æ•°æ®é›†
        mixed_ds = concatenate_datasets(sampled_datasets)

        print(f"âœ… Curriculum dataset built: {len(mixed_ds)} samples")

        return mixed_ds

    target_modules = safe_list(target_modules, ["q_proj", "k_proj", "v_proj", "o_proj"])

    if adapter_name == "lora":
        config = LoraConfig(
            r=lora_r,
            lora_alpha=lora_alpha,
            target_modules=target_modules, 
            lora_dropout=lora_dropout,
            bias="none",
            task_type="CAUSAL_LM",
        )
    elif adapter_name == "adalora":
        config = AdaLoraConfig(
            r=lora_r,
            lora_alpha=lora_alpha,
            target_modules=target_modules,
            lora_dropout=lora_dropout,
            task_type="CAUSAL_LM",
            # ===== AdaLoRA å…³é”®å‚æ•° =====
            init_r=lora_r,
            target_r=lora_r,
            beta1=0.85,
            beta2=0.85,
            tinit=200,
            tfinal=1000,
            deltaT=10,
        )
    elif adapter_name == "bottleneck":
        config = BottleneckConfig(
            bottleneck_size=bottleneck_size,
            non_linearity=non_linearity,
            adapter_dropout=adapter_dropout,
            use_parallel_adapter=use_parallel_adapter,
            use_adapterp=use_adapterp,
            target_modules=target_modules,
            scaling=scaling,
            bias="none",
            task_type="CAUSAL_LM",
        )
    elif adapter_name == "prefix-tuning":
        config = PrefixTuningConfig(
            num_virtual_tokens=num_virtual_tokens,
            task_type="CAUSAL_LM",
        )

    model = get_peft_model(model, config)

    # å…³é”®ï¼šå…ˆåº”ç”¨ PEFTï¼Œå†ç§»åŠ¨åˆ°è®¾å¤‡
    # è¿™æ ·å¯ä»¥ç¡®ä¿ LoRA å‚æ•°æ­£ç¡®åˆå§‹åŒ–å¹¶å¯ç”¨æ¢¯åº¦
    print(f"Process rank {local_rank}: Moving PEFT model to {device}...")
    model = model.to(device)

    # æ¸…ç†ç¼“å­˜
    torch.npu.empty_cache()

    # ç¡®ä¿æ¨¡å‹åœ¨è®­ç»ƒæ¨¡å¼
    model.train()

    # æ˜¾å¼ç¡®ä¿ LoRA å‚æ•°å¯ç”¨æ¢¯åº¦
    for param in model.parameters():
        if param.requires_grad:
            # ç¡®ä¿æ¢¯åº¦å·²å¯ç”¨çš„å‚æ•°ä¿æŒå¯ç”¨çŠ¶æ€
            param.requires_grad = True

    print(f"Process rank {local_rank}: PEFT model ready on {device}")

    # --- å¯ç”¨æ¢¯åº¦æ£€æŸ¥ç‚¹ä»¥èŠ‚çœæ˜¾å­˜ ---
    # æ³¨æ„ï¼šåœ¨ NPU ä¸Šï¼Œæ¢¯åº¦æ£€æŸ¥ç‚¹å¯èƒ½ä¸ PEFT ä¸å…¼å®¹ï¼Œæš‚æ—¶ç¦ç”¨
    use_gradient_checkpointing = False  # å¼ºåˆ¶ç¦ç”¨ï¼Œé¿å…æ¢¯åº¦é—®é¢˜
    if use_gradient_checkpointing:
        # å¿…é¡»åœ¨ PEFT åŒ…è£…åå¯ç”¨ï¼Œè¿™æ · LoRA å±‚ä¹Ÿèƒ½å—ç›Š
        # NPU å…¼å®¹æ€§æ£€æŸ¥ï¼šgradient checkpointing åœ¨ NPU ä¸Šé€šå¸¸å¯ç”¨ï¼Œä½†éœ€è¦æµ‹è¯•
        try:
            if hasattr(model, "gradient_checkpointing_enable"):
                model.gradient_checkpointing_enable()
                print(f"âœ… Gradient checkpointing enabled (saves ~30-50% memory)")
                print(f"   Note: If you encounter errors, try --use_gradient_checkpointing False")
            elif hasattr(model, "gradient_checkpointing"):
                # æŸäº›æ¨¡å‹ä½¿ç”¨ä¸åŒçš„å±æ€§å
                model.gradient_checkpointing = True
                print(f"âœ… Gradient checkpointing enabled via gradient_checkpointing attribute")
            else:
                print("âš ï¸  Warning: Model does not support gradient checkpointing, disabling...")
                use_gradient_checkpointing = False
        except Exception as e:
            print(f"âš ï¸  Warning: Failed to enable gradient checkpointing on NPU: {e}")
            print(f"   Disabling gradient checkpointing. If memory issues persist, try reducing batch_size.")
            use_gradient_checkpointing = False
    else:
        print("â„¹ï¸  Gradient checkpointing disabled (recommended for NPU + PEFT compatibility)")

    # æ‰“å°å¯è®­ç»ƒå‚æ•°ï¼Œç¡®è®¤ LoRA æŒ‚è½½æ­£ç¡®
    model.print_trainable_parameters()

    # ### ä¿®æ”¹ç‚¹ 4: ç§»é™¤æ‰‹åŠ¨çš„ model.bfloat16()ï¼Œä¾èµ– Trainer çš„å‚æ•°æ§åˆ¶
    # å› ä¸º Trainer ä¼šæ ¹æ® args.bf16 è‡ªåŠ¨å¤„ç†ï¼Œæ‰‹åŠ¨è½¬æ¢æœ‰æ—¶ä¼šæ‰°ä¹± Trainer çš„çŠ¶æ€
    
    data = load_dataset("json", data_files=data_path) if data_path.endswith(".json") else load_dataset(data_path)

    # === Curriculum Learning Data Preparation ===
    explain_ds = None
    reasoning_ds = None
    topology_ds = None

    if use_curriculum:
        print("\n" + "="*50)
        print("ğŸ“š Starting Curriculum Learning Setup")
        print("Filtering dataset by difficulty levels...")
        print("="*50 + "\n")

        full_ds = data["train"]

        # æ£€æŸ¥æ•°æ®é›†ä¸­æ˜¯å¦æœ‰ difficulty å­—æ®µ
        if "difficulty" not in full_ds.column_names:
            print("âš ï¸  Warning: 'difficulty' field not found in dataset. Disabling curriculum learning.")
            use_curriculum = False
            train_data = full_ds.shuffle().map(generate_and_tokenize_prompt)
        else:
            # æŒ‰éš¾åº¦è¿‡æ»¤æ•°æ®é›†
            explain_ds = full_ds.filter(lambda x: x.get("difficulty") == "explain")
            reasoning_ds = full_ds.filter(lambda x: x.get("difficulty") == "reasoning")
            topology_ds = full_ds.filter(lambda x: x.get("difficulty") == "topology")

            # å¦‚æœæ²¡æœ‰ topology æ•°æ®ï¼Œä½¿ç”¨ reasoning çš„ä¸€éƒ¨åˆ†
            if len(topology_ds) == 0:
                print("âš ï¸  No 'topology' difficulty data found. Using reasoning data for topology stage.")
                topology_ds = reasoning_ds

            print(f"âœ… Dataset split by difficulty:")
            print(f"   - Explain: {len(explain_ds)} samples")
            print(f"   - Reasoning: {len(reasoning_ds)} samples")
            print(f"   - Topology: {len(topology_ds)} samples")
            print(f"   - Total: {len(full_ds)} samples\n")

            # å…³é”®ä¼˜åŒ–ï¼šå…ˆ tokenize å„ä¸ªå­æ•°æ®é›†ï¼Œå† interleave
            # è¿™æ ·å¯ä»¥æ˜¾ç¤ºè¿›åº¦æ¡ï¼Œè€Œä¸”åªéœ€è¦ tokenize ä¸€æ¬¡
            print(f"ğŸ”„ Pre-tokenizing datasets (this will be done once)...")
            explain_ds = explain_ds.map(generate_and_tokenize_prompt, batched=False, num_proc=1, desc="Tokenizing Explain")
            reasoning_ds = reasoning_ds.map(generate_and_tokenize_prompt, batched=False, num_proc=1, desc="Tokenizing Reasoning")
            topology_ds = topology_ds.map(generate_and_tokenize_prompt, batched=False, num_proc=1, desc="Tokenizing Topology")
            print(f"âœ… Pre-tokenization complete!\n")
    else:
        # é curriculum æ¨¡å¼ï¼šç›´æ¥å¤„ç†æ•°æ®
        print(f"ğŸ”„ Tokenizing dataset (this may take a moment)...")
        train_data = data["train"].shuffle().map(
            generate_and_tokenize_prompt,
            batched=False,
            num_proc=1,
            desc="Tokenizing"
        )
        print(f"âœ… Tokenization complete!")

    gradient_accumulation_steps = (batch_size // micro_batch_size) // world_size

    # æ ¹æ®æ˜¯å¦ä½¿ç”¨ curriculum learning å†³å®šåˆå§‹è®­ç»ƒæ•°æ®
    if use_curriculum:
        # Curriculum learning: å…ˆç”¨ç¬¬ä¸€ä¸ª epoch çš„æ•°æ®ï¼ˆprogress=0ï¼‰
        initial_progress = 0.0
        initial_mixed_ds = build_curriculum_dataset(explain_ds, reasoning_ds, topology_ds, initial_progress)
        # æ•°æ®å·²ç» tokenizedï¼Œåªéœ€è¦ shuffle
        initial_train_data = initial_mixed_ds.shuffle()
    else:
        print(f"ğŸ”„ Tokenizing dataset (this may take a moment)...")
        initial_train_data = train_data.map(
            generate_and_tokenize_prompt,
            batched=False,
            num_proc=1,
            desc="Tokenizing"
        )
        print(f"âœ… Tokenization complete! Ready to train.")

    # ### ä¿®æ”¹ç‚¹ 5: æ ¸å¿ƒä¿®å¤ - å¼€å¯ bf16=True
    # è¿™ä¼šå‘Šè¯‰ Trainer ä¸è¦ä½¿ç”¨ GradScalerï¼Œå› ä¸º BF16 ä¸éœ€è¦ç¼©æ”¾ã€‚
    # å½»åº•è§£å†³ "Loss scaler reducing loss scale to 0.0" é—®é¢˜

    # è‡ªå®šä¹‰å›è°ƒï¼šæ‰“å°è®­ç»ƒè¿›åº¦å’ŒæŒ‡æ ‡
    class ProgressCallback(transformers.TrainerCallback):
        def on_log(self, _args, state, _control, logs=None, **_kwargs):
            """æ¯æ¬¡æ—¥å¿—è®°å½•æ—¶è°ƒç”¨"""
            if logs and state.is_local_process_zero:
                # åªåœ¨ä¸»è¿›ç¨‹æ‰“å°
                step = state.global_step
                epoch = state.epoch if state.epoch is not None else 0

                # æ„å»ºæ—¥å¿—ä¿¡æ¯
                log_str = f"[Step {step} | Epoch {epoch:.2f}]"

                if "loss" in logs:
                    log_str += f" Loss: {logs['loss']:.4f}"
                if "learning_rate" in logs:
                    log_str += f" | LR: {logs['learning_rate']:.2e}"
                if "grad_norm" in logs:
                    log_str += f" | Grad Norm: {logs['grad_norm']:.4f}"

                # æ‰“å°åˆ°æ§åˆ¶å°
                if "loss" in logs:  # åªåœ¨æœ‰ loss æ—¶æ‰“å°ï¼Œé¿å…é‡å¤
                    print(log_str)

        def on_epoch_end(self, _args, state, _control, **_kwargs):
            """æ¯ä¸ª epoch ç»“æŸæ—¶è°ƒç”¨"""
            if state.is_local_process_zero:
                print(f"\n{'='*60}")
                print(f"âœ… Epoch {int(state.epoch)} completed!")
                print(f"   Total steps: {state.global_step}")
                print(f"{'='*60}\n")

    trainer = transformers.Trainer(
        model=model,
        train_dataset=initial_train_data,
        args=transformers.TrainingArguments(
            per_device_train_batch_size=micro_batch_size,
            gradient_accumulation_steps=gradient_accumulation_steps,
            warmup_steps=100,
            num_train_epochs=1 if use_curriculum else num_epochs,  # Curriculum: æ¯æ¬¡è®­ç»ƒ1ä¸ªepoch
            learning_rate=learning_rate,

            # --- å…³é”®ä¿®æ”¹ ---
            bf16=True,      # å¿…é¡»å¼€å¯ï¼è¿™ä¼šç¦ç”¨ FP16 GradScaler
            fp16=False,     # ç¡®ä¿å…³é—­ FP16
            optim="adamw_torch",
            gradient_checkpointing=use_gradient_checkpointing,  # æ¢¯åº¦æ£€æŸ¥ç‚¹ï¼ŒèŠ‚çœæ˜¾å­˜
            # ----------------

            # --- æ—¥å¿—å’Œè¿›åº¦æ˜¾ç¤ºé…ç½® ---
            logging_strategy="steps",
            logging_steps=10,
            logging_first_step=True,  # æ˜¾ç¤ºç¬¬ä¸€æ­¥çš„æŒ‡æ ‡
            disable_tqdm=False,  # å¯ç”¨è¿›åº¦æ¡
            # ----------------

            dataloader_pin_memory=False,
            save_strategy="steps",
            save_steps=save_step,
            output_dir=output_dir,
            ddp_find_unused_parameters=False,
            report_to="none",

        ),
        data_collator=transformers.DataCollatorForSeq2Seq(tokenizer, pad_to_multiple_of=8, padding=True),
        callbacks=[ProgressCallback()],  # æ·»åŠ è‡ªå®šä¹‰å›è°ƒ
    )

    # ç¦ç”¨ KV cacheï¼ˆè®­ç»ƒæ—¶ä¸éœ€è¦ï¼‰
    model.config.use_cache = False

    # æ¸…ç†å†…å­˜
    torch.npu.empty_cache()

    # =================================================================
    # Training Loop with Curriculum Learning Support
    # =================================================================
    if use_curriculum:
        print("\n" + "="*50)
        print("ğŸ“ Starting Curriculum Learning Training")
        print(f"Total Epochs: {num_epochs}")
        print("="*50 + "\n")

        for epoch in range(num_epochs):
            # è®¡ç®—å½“å‰è®­ç»ƒè¿›åº¦
            progress = epoch / num_epochs

            print(f"\n{'='*50}")
            print(f"ğŸ“– Epoch {epoch + 1}/{num_epochs} (Progress: {progress:.2%})")
            print(f"{'='*50}\n")

            # åŠ¨æ€æ„å»ºå½“å‰ epoch çš„æ•°æ®é›†
            mixed_ds = build_curriculum_dataset(explain_ds, reasoning_ds, topology_ds, progress)
            # æ•°æ®å·²ç» tokenizedï¼Œåªéœ€è¦ shuffle
            current_train_data = mixed_ds.shuffle()

            # æ›´æ–° trainer çš„è®­ç»ƒæ•°æ®é›†
            trainer.train_dataset = current_train_data

            # è®­ç»ƒå½“å‰ epoch
            if epoch == 0:
                # ç¬¬ä¸€ä¸ª epochï¼Œå¯èƒ½éœ€è¦ä» checkpoint æ¢å¤
                trainer.train(resume_from_checkpoint=resume_from_checkpoint)
            else:
                # åç»­ epochï¼šä¸ä» checkpoint æ¢å¤ï¼Œç›´æ¥ç»§ç»­è®­ç»ƒ
                # æ¨¡å‹å·²ç»åœ¨å†…å­˜ä¸­å¹¶ä¸”å·²è®­ç»ƒï¼Œä¸éœ€è¦é‡æ–°åŠ è½½
                # æ³¨æ„ï¼šä¼ å…¥ None è€Œä¸æ˜¯ Trueï¼Œé¿å…å°è¯•åŠ è½½ checkpoint
                trainer.train(resume_from_checkpoint=None)

            print(f"\nâœ… Epoch {epoch + 1}/{num_epochs} completed!\n")

        print("\n" + "="*50)
        print("ğŸ‰ Curriculum Learning Training Completed!")
        print("="*50 + "\n")
    else:
        # æ ‡å‡†è®­ç»ƒï¼ˆä¸ä½¿ç”¨ curriculum learningï¼‰
        trainer.train(resume_from_checkpoint=resume_from_checkpoint)

    model.save_pretrained(output_dir)

if __name__ == "__main__":
    fire.Fire(train)