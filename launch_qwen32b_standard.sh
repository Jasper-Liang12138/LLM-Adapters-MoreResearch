#!/bin/bash

# ============================================
# å¤©ç¿¼äº‘è®­æ¨æœåŠ¡ - Qwen-32B æ ‡å‡†å¾®è°ƒï¼ˆæ— è¯¾ç¨‹å­¦ä¹ ï¼‰
# é€‚ç”¨äºå¹³å°è‡ªåŠ¨é…ç½®ç¯å¢ƒå˜é‡çš„åœºæ™¯
# ============================================

# æ¨¡å‹å’Œæ•°æ®è·¯å¾„
BASE_MODEL="Qwen/Qwen-32B"
DATA_PATH="/work/basicData/2021154936252485632"
OUTPUT_DIR="/work/mount/Qwen32bLoraSft"

# è®­ç»ƒè¶…å‚æ•°
BATCH_SIZE=128
MICRO_BATCH_SIZE=1
NUM_EPOCHS=3
LEARNING_RATE=2e-5
CUTOFF_LEN=2048

# LoRA å‚æ•°
LORA_R=64
LORA_ALPHA=128
LORA_DROPOUT=0.05

# DeepSpeed é…ç½®æ–‡ä»¶
DS_CONFIG="./ds_config_zero3.json"

echo "ğŸš€ Starting standard fine-tuning on å¤©ç¿¼äº‘..."
echo "ğŸ“Š Environment Info:"
echo "   - MASTER_ADDR: ${MASTER_ADDR:-auto}"
echo "   - MASTER_PORT: ${MASTER_PORT:-auto}"
echo "   - WORLD_SIZE: ${WORLD_SIZE:-auto}"
echo "   - RANK: ${RANK:-auto}"
echo "   - LOCAL_RANK: ${LOCAL_RANK:-auto}"

# ç›´æ¥è¿è¡Œè®­ç»ƒè„šæœ¬ï¼ˆDeepSpeed ä¼šè¯»å–ç¯å¢ƒå˜é‡ï¼‰
python -m torch.distributed.launch \
    --use_env \
    finetune_npu_deepspeed_standard.py \
    --base_model "$BASE_MODEL" \
    --data_path "$DATA_PATH" \
    --output_dir "$OUTPUT_DIR" \
    --adapter_name lora \
    --batch_size $BATCH_SIZE \
    --micro_batch_size $MICRO_BATCH_SIZE \
    --num_epochs $NUM_EPOCHS \
    --learning_rate $LEARNING_RATE \
    --cutoff_len $CUTOFF_LEN \
    --lora_r $LORA_R \
    --lora_alpha $LORA_ALPHA \
    --lora_dropout $LORA_DROPOUT \
    --target_modules '["q_proj", "k_proj", "v_proj", "o_proj", "gate_proj", "up_proj", "down_proj"]' \
    --train_on_inputs False \
    --deepspeed_config "$DS_CONFIG"

# ============================================
# ä½¿ç”¨è¯´æ˜:
#
# 1. åœ¨å¤©ç¿¼äº‘è®­æ¨æœåŠ¡ç½‘é¡µç•Œé¢:
#    - é€‰æ‹©è®­ç»ƒä»»åŠ¡ç±»å‹: åˆ†å¸ƒå¼è®­ç»ƒ
#    - èŠ‚ç‚¹æ•°: 2
#    - æ¯èŠ‚ç‚¹ NPU æ•°: 8
#    - å¯åŠ¨è„šæœ¬: bash launch_qwen32b_standard.sh
#
# 2. å¹³å°ä¼šè‡ªåŠ¨åœ¨æ¯ä¸ªèŠ‚ç‚¹ä¸Šæ‰§è¡Œæ­¤è„šæœ¬
# 3. ç¯å¢ƒå˜é‡ç”±å¹³å°è‡ªåŠ¨æ³¨å…¥
#
# æ³¨æ„: æ­¤ç‰ˆæœ¬ä¸åŒ…å«è¯¾ç¨‹å­¦ä¹ åŠŸèƒ½ï¼Œé€‚ç”¨äºæ ‡å‡†æ•°æ®é›†
# ============================================
