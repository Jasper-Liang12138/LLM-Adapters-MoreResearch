import os
import sys

# ### NPU ç¯å¢ƒå˜é‡è®¾ç½®
os.environ["ACL_PRECISION_MODE"] = "must_keep_origin_dtype"
os.environ["LCCL_DETERMINISTIC"] = "1"
os.environ["HCC_DETERMINISTIC"] = "1"
os.environ["PYTHONWARNINGS"] = "ignore"
os.environ["TOKENIZERS_PARALLELISM"] = "false"

from typing import List, Optional, Union
import json
import fire
import torch
import torch_npu
from torch_npu.contrib import transfer_to_npu
import time

# NPU è®¾ç½®
torch_npu.npu.set_compile_mode(jit_compile=False)
torch.npu.set_option({"ACL_PRECISION_MODE": "must_keep_origin_dtype"})

import transformers
from datasets import load_dataset
from tqdm import tqdm

sys.path.append(os.path.join(os.getcwd(), "peft/src/"))
from peft import (
    LoraConfig, get_peft_model
)
from transformers import AutoModelForCausalLM, AutoTokenizer

def safe_list(value, default=None):
    if default is None: default = []
    if value is None: return default
    if isinstance(value, list): return value
    return default

def train(
        base_model: str = "",
        data_path: str = "",
        output_dir: str = "./qwen32b-lora",
        adapter_name: str = "lora",
        batch_size: int = 128,
        micro_batch_size: int = 1,
        num_epochs: int = 3,
        learning_rate: float = 2e-5,
        cutoff_len: int = 2048,
        val_set_size: int = 0,
        save_step: int = 500,
        lora_r: int = 64,
        lora_alpha: int = 128,
        lora_dropout: float = 0.05,
        target_modules: List[str] = None,
        train_on_inputs: bool = False,
        resume_from_checkpoint: str = None,
        deepspeed_config: str = None,
        local_rank: int = -1,
):
    """
    ä½¿ç”¨ DeepSpeed ZeRO-3 è®­ç»ƒ Qwen-32B (æ ‡å‡†å¾®è°ƒç‰ˆæœ¬)
    æ”¯æŒå¤šèŠ‚ç‚¹åˆ†å¸ƒå¼è®­ç»ƒ
    """
    print(f"ğŸš€ Finetuning Qwen-32B on Ascend NPU with DeepSpeed ZeRO-3...")

    # DeepSpeed ä¼šè‡ªåŠ¨è®¾ç½®è¿™äº›ç¯å¢ƒå˜é‡
    local_rank = int(os.environ.get("LOCAL_RANK", "0"))
    world_size = int(os.environ.get("WORLD_SIZE", 1))
    rank = int(os.environ.get("RANK", "0"))

    print(f"ğŸ“Š Distributed Info: Rank {rank}/{world_size}, Local Rank {local_rank}")

    # è®¾ç½®å½“å‰è¿›ç¨‹ä½¿ç”¨çš„ NPU
    torch.npu.set_device(local_rank)

    # æ¸…ç†ç¼“å­˜
    torch.npu.empty_cache()

    # å¤šèŠ‚ç‚¹è®­ç»ƒæ—¶ï¼Œé”™å¼€æ¨¡å‹åŠ è½½æ—¶é—´
    if world_size > 1 and rank > 0:
        time.sleep(rank * 2)

    print(f"ğŸ’¾ Loading model: {base_model}")

    # ä½¿ç”¨ DeepSpeed æ—¶ï¼Œæ¨¡å‹åˆå§‹åŒ–åœ¨ CPU ä¸Šï¼ŒDeepSpeed ä¼šå¤„ç†åˆ†ç‰‡
    model = AutoModelForCausalLM.from_pretrained(
        base_model,
        torch_dtype=torch.bfloat16,
        trust_remote_code=True,
        attn_implementation="eager",
        use_cache=False,
    )

    print(f"âœ… Model loaded on rank {rank}")

    # Tokenizer
    tokenizer = AutoTokenizer.from_pretrained(base_model, trust_remote_code=True)
    tokenizer.pad_token = tokenizer.eos_token
    tokenizer.padding_side = "left"

    def generate_and_tokenize_prompt(data_point):
        """æ•°æ®é¢„å¤„ç†å‡½æ•°"""
        messages = [
            {"role": "system", "content": "You are a helpful assistant."},
            {"role": "user", "content": data_point["instruction"] + ("\n" + data_point["input"] if data_point.get("input") else "")},
            {"role": "assistant", "content": data_point["output"]}
        ]

        full_tokens = tokenizer.apply_chat_template(
            messages,
            tokenize=True,
            truncation=True,
            max_length=cutoff_len
        )
        labels = list(full_tokens)

        if not train_on_inputs:
            # åªè®¡ç®— assistant éƒ¨åˆ†çš„ loss
            user_tokens = tokenizer.apply_chat_template(
                messages[:-1],
                tokenize=True,
                add_generation_prompt=True
            )
            user_len = len(user_tokens)
            labels = [-100] * user_len + labels[user_len:]
            if len(labels) > len(full_tokens):
                labels = labels[:len(full_tokens)]

        return {
            "input_ids": full_tokens,
            "attention_mask": [1] * len(full_tokens),
            "labels": labels
        }

    # LoRA é…ç½®
    target_modules = safe_list(target_modules, [
        "q_proj", "k_proj", "v_proj", "o_proj",
        "gate_proj", "up_proj", "down_proj"
    ])

    config = LoraConfig(
        r=lora_r,
        lora_alpha=lora_alpha,
        target_modules=target_modules,
        lora_dropout=lora_dropout,
        bias="none",
        task_type="CAUSAL_LM",
    )

    model = get_peft_model(model, config)
    model.print_trainable_parameters()

    # åŠ è½½æ•°æ®é›†
    if rank == 0:
        print(f"ğŸ“š Loading dataset from {data_path}")
    data = load_dataset("json", data_files=data_path) if data_path.endswith(".json") else load_dataset(data_path)

    # æ ‡å‡†æ•°æ®å¤„ç†
    if rank == 0:
        print(f"ğŸ”„ Tokenizing dataset...")
    train_data = data["train"].shuffle().map(
        generate_and_tokenize_prompt,
        batched=False,
        num_proc=4,
        desc="Tokenizing"
    )
    if rank == 0:
        print(f"âœ… Tokenization complete! Total samples: {len(train_data)}")

    # è®¡ç®—æ¢¯åº¦ç´¯ç§¯æ­¥æ•°
    gradient_accumulation_steps = batch_size // (micro_batch_size * world_size)

    if rank == 0:
        print(f"âš™ï¸  Training Configuration:")
        print(f"   - World Size: {world_size}")
        print(f"   - Micro Batch Size: {micro_batch_size}")
        print(f"   - Gradient Accumulation Steps: {gradient_accumulation_steps}")
        print(f"   - Effective Batch Size: {micro_batch_size * gradient_accumulation_steps * world_size}")

    # DeepSpeed é…ç½®
    if deepspeed_config is None:
        # å¦‚æœæ²¡æœ‰æä¾›é…ç½®æ–‡ä»¶ï¼Œä½¿ç”¨é»˜è®¤é…ç½®
        deepspeed_config = {
            "train_batch_size": batch_size,
            "train_micro_batch_size_per_gpu": micro_batch_size,
            "gradient_accumulation_steps": gradient_accumulation_steps,
            "gradient_clipping": 1.0,
            "zero_optimization": {
                "stage": 3,
                "offload_optimizer": {
                    "device": "cpu",
                    "pin_memory": True
                },
                "offload_param": {
                    "device": "cpu",
                    "pin_memory": True
                },
                "overlap_comm": True,
                "contiguous_gradients": True,
                "sub_group_size": 1e9,
                "reduce_bucket_size": 5e8,
                "stage3_prefetch_bucket_size": 5e8,
                "stage3_param_persistence_threshold": 1e6,
                "stage3_max_live_parameters": 1e9,
                "stage3_max_reuse_distance": 1e9,
                "stage3_gather_16bit_weights_on_model_save": True
            },
            "bf16": {
                "enabled": True
            },
            "steps_per_print": 10,
            "wall_clock_breakdown": False
        }

        # ä¿å­˜é…ç½®åˆ°æ–‡ä»¶
        ds_config_path = os.path.join(output_dir, "ds_config.json")
        os.makedirs(output_dir, exist_ok=True)
        with open(ds_config_path, "w") as f:
            json.dump(deepspeed_config, f, indent=2)
        if rank == 0:
            print(f"ğŸ’¾ DeepSpeed config saved to {ds_config_path}")
        deepspeed_config = ds_config_path

    # è®­ç»ƒå‚æ•°
    training_args = transformers.TrainingArguments(
        output_dir=output_dir,
        per_device_train_batch_size=micro_batch_size,
        gradient_accumulation_steps=gradient_accumulation_steps,
        num_train_epochs=num_epochs,
        learning_rate=learning_rate,
        bf16=True,
        fp16=False,
        logging_steps=10,
        logging_first_step=True,
        save_strategy="steps",
        save_steps=save_step,
        save_total_limit=3,
        dataloader_pin_memory=False,
        dataloader_num_workers=4,
        remove_unused_columns=False,
        ddp_find_unused_parameters=False,
        deepspeed=deepspeed_config,
        report_to="none",
        warmup_steps=100,
        lr_scheduler_type="cosine",
        weight_decay=0.01,
        max_grad_norm=1.0,
    )

    # Trainer
    trainer = transformers.Trainer(
        model=model,
        args=training_args,
        train_dataset=train_data,
        data_collator=transformers.DataCollatorForSeq2Seq(
            tokenizer,
            pad_to_multiple_of=8,
            return_tensors="pt",
            padding=True
        ),
    )

    # å¼€å§‹è®­ç»ƒ
    if rank == 0:
        print(f"\n{'='*60}")
        print(f"ğŸ¯ Starting Training...")
        print(f"{'='*60}\n")

    trainer.train(resume_from_checkpoint=resume_from_checkpoint)

    # ä¿å­˜æ¨¡å‹ï¼ˆåªåœ¨ rank 0 ä¿å­˜ï¼‰
    if rank == 0:
        print(f"\nğŸ’¾ Saving model to {output_dir}")
        model.save_pretrained(output_dir)
        tokenizer.save_pretrained(output_dir)
        print(f"âœ… Training completed!")

if __name__ == "__main__":
    fire.Fire(train)
